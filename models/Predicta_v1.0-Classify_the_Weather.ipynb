{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "daily_data = pd.read_csv('../artifacts/daily_data.csv')\n",
    "\n",
    "# Separate the rows with and without missing condition_text\n",
    "train_data = daily_data.dropna(subset=['condition_text'])\n",
    "test_data = daily_data[daily_data['condition_text'].isnull()]\n",
    "\n",
    "# Combine the data to ensure consistent encoding\n",
    "combined_data = pd.concat([train_data, test_data])\n",
    "\n",
    "# Encode categorical features\n",
    "label_encoder = LabelEncoder()\n",
    "combined_data['city_id'] = label_encoder.fit_transform(combined_data['city_id'])\n",
    "\n",
    "# Convert sunrise and sunset to total minutes from midnight\n",
    "def convert_to_minutes(time_str):\n",
    "    time_parts = time_str.strip().split(' ')\n",
    "    period = time_parts[1]\n",
    "    hours, minutes = map(int, time_parts[0].split(':'))\n",
    "    if period == 'PM' and hours != 12:\n",
    "        hours += 12\n",
    "    elif period == 'AM' and hours == 12:\n",
    "        hours = 0\n",
    "    return hours * 60 + minutes\n",
    "\n",
    "combined_data['sunrise'] = combined_data['sunrise'].apply(convert_to_minutes)\n",
    "combined_data['sunset'] = combined_data['sunset'].apply(convert_to_minutes)\n",
    "\n",
    "# Feature engineering: Adding new features or interactions\n",
    "combined_data['temp_range'] = combined_data['sunset'] - combined_data['sunrise']\n",
    "combined_data['wind_product'] = combined_data['wind_kph'] * combined_data['gust_kph']\n",
    "\n",
    "# Split the combined data back into train and test sets\n",
    "train_data = combined_data[combined_data['condition_text'].notna()].copy()\n",
    "test_data = combined_data[combined_data['condition_text'].isna()].copy()\n",
    "\n",
    "# Impute missing values (if any) in numerical columns using median\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "train_data_imputed = pd.DataFrame(imputer.fit_transform(train_data.drop(columns=['day_id', 'condition_text'])), columns=train_data.columns.drop(['day_id', 'condition_text']))\n",
    "test_data_imputed = pd.DataFrame(imputer.transform(test_data.drop(columns=['day_id', 'condition_text'])), columns=test_data.columns.drop(['day_id', 'condition_text']))\n",
    "\n",
    "# Normalize the numerical features\n",
    "scaler = StandardScaler()\n",
    "train_data_scaled = pd.DataFrame(scaler.fit_transform(train_data_imputed), columns=train_data_imputed.columns)\n",
    "test_data_scaled = pd.DataFrame(scaler.transform(test_data_imputed), columns=test_data_imputed.columns)\n",
    "\n",
    "# Encode the target variable\n",
    "target_encoder = LabelEncoder()\n",
    "train_data['condition_text'] = target_encoder.fit_transform(train_data['condition_text'])\n",
    "\n",
    "# Prepare the final training data\n",
    "X_train = train_data_scaled\n",
    "y_train = train_data['condition_text']\n",
    "\n",
    "# Define the models with a larger set of hyperparameters for tuning\n",
    "rf_model = RandomForestClassifier()\n",
    "gb_model = GradientBoostingClassifier()\n",
    "xgb_model = XGBClassifier()\n",
    "\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'max_depth': [5, 7, 9],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "param_grid_gb = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'subsample': [0.8, 0.9, 1.0]\n",
    "}\n",
    "\n",
    "param_grid_xgb = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'subsample': [0.8, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.8, 0.9, 1.0]\n",
    "}\n",
    "\n",
    "grid_search_rf = GridSearchCV(rf_model, param_grid_rf, refit=True, verbose=1, n_jobs=-1, cv=5)\n",
    "grid_search_gb = GridSearchCV(gb_model, param_grid_gb, refit=True, verbose=1, n_jobs=-1, cv=5)\n",
    "grid_search_xgb = GridSearchCV(xgb_model, param_grid_xgb, refit=True, verbose=1, n_jobs=-1, cv=5)\n",
    "\n",
    "# Train the models\n",
    "grid_search_rf.fit(X_train, y_train)\n",
    "grid_search_gb.fit(X_train, y_train)\n",
    "grid_search_xgb.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters from GridSearchCV\n",
    "print(f\"Best Parameters RF: {grid_search_rf.best_params_}\")\n",
    "print(f\"Best Parameters GB: {grid_search_gb.best_params_}\")\n",
    "print(f\"Best Parameters XGB: {grid_search_xgb.best_params_}\")\n",
    "\n",
    "# Train the final models with the best parameters\n",
    "best_rf_model = grid_search_rf.best_estimator_\n",
    "best_gb_model = grid_search_gb.best_estimator_\n",
    "best_xgb_model = grid_search_xgb.best_estimator_\n",
    "\n",
    "# Create an ensemble model\n",
    "voting_clf = VotingClassifier(estimators=[\n",
    "    ('rf', best_rf_model),\n",
    "    ('gb', best_gb_model),\n",
    "    ('xgb', best_xgb_model)\n",
    "], voting='hard')\n",
    "\n",
    "voting_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the missing condition_text values\n",
    "test_data['condition_text'] = target_encoder.inverse_transform(voting_clf.predict(test_data_scaled))\n",
    "\n",
    "# Ensure the predicted values are filled back into the original dataset\n",
    "daily_data.loc[daily_data['condition_text'].isnull(), 'condition_text'] = test_data['condition_text'].values\n",
    "\n",
    "# Create the submission dataframe with all day_id and condition_text\n",
    "submission_df = daily_data[['day_id', 'condition_text']]\n",
    "\n",
    "# Save the submission dataframe in the current working directory\n",
    "submission_df.to_csv('../artifacts/submission.csv', index=False)\n",
    "print(\"Submission file saved as submission.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
